#This file uses machine learning models to analyze the Medical Appointment No Shows dataset
#this dataset can be found on Kaggle: https://www.kaggle.com/joniarroba/noshowappointments/data
#the dataset is to be used to determine whether a patient will show up or not to a medical appointment
#It is coded in Python 3.6.2

import pandas as pd
import numpy as np
import math
from pandas import DataFrame
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
import xgboost

df = pd.read_csv('noshow_data.csv',sep=',')
df = df.drop(['PatientId','AppointmentID'], axis=1)

print('there are ',df.shape[0],' people in the data set')

#fix typos
df.rename(columns = {'Hipertension':'Hypertension', 'Handcap': 'Handicap'}, inplace = True)
print(df.columns)

df.ScheduledDay = df.ScheduledDay.apply(np.datetime64)
df.AppointmentDay = df.AppointmentDay.apply(np.datetime64)

#add days to appointment attribute
daysToAppointment = df.AppointmentDay - df.ScheduledDay
daysToAppointment = daysToAppointment.apply(lambda x: x.total_seconds() / (3600 * 24))
daysToAppointment = round(daysToAppointment+.5)
daysToAppointment = daysToAppointment.astype(np.float)
df = df.assign(DaysToAppointment=daysToAppointment)
df['DaysToAppointment'] = df['DaysToAppointment'].astype(int)

#add in hour appointment was booked during the day, sadly looking at the data we don't know the time the appointment
#was booked for
def calculateHour(timestamp):
    timestamp = str(timestamp)
    hour = int(timestamp[11:13])
    minute = int(timestamp[14:16])
    second = int(timestamp[17:])
    return round(hour + minute/60 + second/3600)

df['HourOfTheDay'] = df.ScheduledDay.apply(calculateHour)

#add in month of appointment (possible weather affects)
def calculateMonth(timestamp):
    timestamp = str(timestamp)
    month = int(timestamp[5:7])
    return round(month)

df['MonthofAppointment'] = df.AppointmentDay.apply(calculateMonth)
print(df.head())

#check for missing values, and check unique values for each feature in dataset
print('Age:',sorted(df.Age.unique()))
print('Gender:',df.Gender.unique())
print('Diabetes:',df.Diabetes.unique())
print('Alcoholism:',df.Alcoholism.unique())
print('Hypertension:',df.Hypertension.unique())
print('Handicap:',df.Handicap.unique())
print('Scholarship:',df.Scholarship.unique())
print('SMS_received:',df.SMS_received.unique())
print('HourOfTheDay:', sorted(df.HourOfTheDay.unique()))
print('DaysToAppointment:', sorted(df.DaysToAppointment.unique()))
print('MonthofAppointment:', sorted(df.MonthofAppointment.unique()))

#Ages -1 and above 100 are going to be considered outliers
#DaystoAppointment -1 and -6 are also outliers
df = df[(df.Age >= 0) & (df.Age <= 100) & (df.DaysToAppointment >= 0)]
df_out = df['No-show']
df_in = df.drop(['No-show'], axis=1)

print('there are ',df_in.shape[0],' people in the data set')

#The appointments are only for the months of April, May and June, so we are going to remove that feature
#We will remove the features with dates and times
df_in = df_in.drop(['ScheduledDay','AppointmentDay','MonthofAppointment'], axis=1)
df_in2 = df_in.drop(['Neighbourhood'], axis=1)#we may need to drop the Neighbourhood feature too

#Now we need to encode the following features: Gender, and Neighbourhood (the latter will need hot encoding)

from sklearn.preprocessing import LabelEncoder#this is used for encoding categorical values to 0,1,2... (in order)
gender_le = LabelEncoder()#encoding M and F into numbers in Gender
neigh_le = LabelEncoder()#encoding hospital into numbers
noshow_le = LabelEncoder()#encoding Yes or No: noshow values into numbers

X = df_in.values
X2 = df_in2.values#in case dropping the Neighbourhood feature helps

Y_out = df_out.values

X[:, 0] = gender_le.fit_transform(X[:, 0])#encoding gender feature so male->1, female->0
X2[:, 0] = X[:, 0]#encoding gender feature so male->1, female->0
print(X[0:5, :])

X[:, 2] = neigh_le.fit_transform(X[:, 2])#
#data_label_encoded2 = embarked_le.fit_transform(df['Embarked'])
#df['Embarked'] = data_label_encoded2
#print(df.head())
print('The first 5 rows of our dataset without one hot encoding are:')
print(X[0:5, :])
print('Neighbourhood:', sorted(df_in.Neighbourhood.unique()))
print(len(sorted(df_in.Neighbourhood.unique())))

from sklearn.preprocessing import OneHotEncoder#this is used for unordered categorical values
ohe = OneHotEncoder(categorical_features=[2])#we need to encode Neighbourhood because there it has no inherent order
X = ohe.fit_transform(X).toarray()#note, when you do this the one hot encoding goes to the first columns of the array
X = np.delete(X, 0, axis=1)#we delete the first column of X because the information can be inferred by the second
    # two columns

Y_out = noshow_le.fit_transform(Y_out)#encoding gender feature so Malignant (Yes)->1, Benign (No)->0


#Scaling
stdsc = StandardScaler()
X_std = stdsc.fit_transform(X)#standardize the values of our input set
X2_std = stdsc.fit_transform(X2)#standardize the values of our input set


#now we are going to split our training data into training and testing data, the reason for this is so that we can
#test for overfitting and such
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_std, Y_out, test_size = 0.2, random_state = 1,\
    stratify = Y_out)#this makes sure the training and test datasets have same class proportions
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2_std, Y_out, test_size = 0.2, random_state = 1,\
        stratify = Y_out)#this makes sure the training and test datasets have same class proportions

#######################################################################################################
#######################################################################################################
#######################################################################################################
#define function which prints scores of model given train/test split
def modelscores(model, X_train, X_test, y_train, y_test, X_std, Y_out):
    scores = cross_val_score(estimator=model, X=X_std, y=Y_out, cv=10, n_jobs=1)
    print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))
    model.fit(X_train, y_train)
    print('Test accuracy: %.3f' % model.score(X_test, y_test))

#######################################################################################################
#######################################################################################################
#######################################################################################################


'''#Logistic Regression Estimator

pipe_lr = make_pipeline(PCA(), LogisticRegression(random_state=0))
print(pipe_lr.get_params().keys())
param_range = [.1, .5, 1, 10]#regularization parameter for logistic regression

def lrpca(X_train, y_train):
    n_components=np.arange(7,X_train.shape[1]+1,1)#the number of components PCA reduces the data to
    param_grid = [{'pca__n_components': n_components, 'logisticregression__C': param_range}]
    gslr = GridSearchCV(estimator= pipe_lr,\
        param_grid=param_grid,\
        scoring='accuracy',\
        cv=10)

    gslr = gslr.fit(X_train, y_train)

    print('For Logistic Regression')
    print('The best regulation parameter choice is: ',gslr.best_params_)
    print('The best score is: %.3f' % (gslr.best_score_))
    lreg_best = gslr.best_estimator_ #uses the best parameter values found via grid search
    return lreg_best

print('When not using one hot encoding')
lrbest = lrpca(X_train2, y_train2)
modelscores(lrbest, X_train2, X_test2, y_train2, y_test2, X2_std, Y_out)'''


#######################################################################################################
#######################################################################################################
#######################################################################################################

'''#Random Forest Estimator
forest = RandomForestClassifier(random_state=1)

#n_estlist = [10, 25, 50, 100]
param_grid_rf = { \
    'n_estimators': [5, 10, 25, 50 ],\
    'max_features': ['auto', 'sqrt', 'log2'],\
    'max_depth' : [2,3,4,5,6,7,8],\
    'criterion' :['gini', 'entropy']}

gsrf = GridSearchCV(estimator=forest,param_grid=param_grid_rf,scoring='accuracy',cv=10,n_jobs=2)

gsrf = gsrf.fit(X_train2, y_train2)
print('For Random Forest')
print('The best parameter choice is: ',gsrf.best_params_)
print('The best score is: %.3f' % (gsrf.best_score_))

rforest_best = gsrf.best_estimator_ #uses the best parameter values found via grid search
modelscores(rforest_best, X_train2, X_test2, y_train2, y_test2, X2_std, Y_out)'''

#######################################################################################################
#######################################################################################################
#######################################################################################################

'''#AdaBoost using Tree Stumps (also just tree stump)

tree_stump = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=1)
ada = AdaBoostClassifier(base_estimator=tree_stump, n_estimators = 500, random_state=1)

tree_stump = tree_stump.fit(X_train2, y_train2)
y_train_pred = tree_stump.predict(X_train2)
y_test_pred = tree_stump.predict(X_test2)
tree_train = accuracy_score(y_train2, y_train_pred)
tree_test = accuracy_score(y_test2, y_test_pred)
print('For tree stump:')
print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train, tree_test))


def adamodel(X_train, y_train):
    param_grid_ada = { 'learning_rate' : [0.05, 0.1, 0.2]}
    gsada = GridSearchCV(estimator = ada, param_grid = param_grid_ada, scoring='accuracy', cv=10)
    gsada = gsada.fit(X_train, y_train)
    print('For AdaBoost using Tree Stumps')
    print('The best parameter choice is: ',gsada.best_params_)
    print('The best score is: %.3f' % (gsada.best_score_))
    ada_best = gsada.best_estimator_  # uses the best parameter values found via grid search
    return ada_best

ada_best = adamodel(X_train2, y_train2)
modelscores(ada_best, X_train2, X_test2, y_train2, y_test2, X2_std, Y_out)'''


#######################################################################################################
#######################################################################################################
#######################################################################################################

#K-Nearest Neighbours
#remember we have already used standard scaling on the data, but we will want to implement PCA

pipe_knn = make_pipeline(PCA(), KNeighborsClassifier(p=2, metric='minkowski'))
print(pipe_knn.get_params().keys())

def knnmodel(X_train, y_train):
    n_neighbors=np.arange(1, 11, 1)#the number of nearest neighbours considered
    n_components=np.arange(1, X_train.shape[1]+1, 1)#the number of components PCA reduces the data to
    param_grid4 = [{'pca__n_components': n_components, 'kneighborsclassifier__n_neighbors': n_neighbors}]
    gsknnpca = GridSearchCV(estimator=pipe_knn, param_grid=param_grid4, scoring='accuracy', cv=10)

    gsknnpca = gsknnpca.fit(X_train, y_train)
    print('For K-Nearest Neighbours with PCA')
    print('The best parameter choice is: ',gsknnpca.best_params_)
    print('The best score is: %.3f' % (gsknnpca.best_score_))

    knnpca_best = gsknnpca.best_estimator_ #uses the best parameter values found via grid search
    return knnpca_best

knnpca_best = knnmodel(X_train2, y_train2)
modelscores(knnpca_best, X_train2, X_test2, y_train2, y_test2, X2_std, Y_out)


