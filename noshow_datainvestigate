#This file uses machine learning models to analyze the Medical Appointment No Shows dataset
#this dataset can be found on Kaggle: https://www.kaggle.com/joniarroba/noshowappointments/data
#the dataset is to be used to determine whether a patient will show up or not to a medical appointment
#It is coded in Python 3.6.2

import pandas as pd
import numpy as np
import math
from pandas import DataFrame
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
import xgboost

df = pd.read_csv('noshow_data.csv',sep=',')
df = df.drop(['PatientId','AppointmentID'], axis=1)

print('there are ',df.shape[0],' people in the data set')

#fix typos
df.rename(columns = {'Hipertension':'Hypertension', 'Handcap': 'Handicap'}, inplace = True)
print(df.columns)

df.ScheduledDay = df.ScheduledDay.apply(np.datetime64)
df.AppointmentDay = df.AppointmentDay.apply(np.datetime64)

#add days to appointment attribute
daysToAppointment = df.AppointmentDay - df.ScheduledDay
daysToAppointment = daysToAppointment.apply(lambda x: x.total_seconds() / (3600 * 24))
daysToAppointment = round(daysToAppointment+.5)
daysToAppointment = daysToAppointment.astype(np.int64)
df = df.assign(DaysToAppointment=daysToAppointment)

#add in hour appointment was booked during the day, sadly looking at the data we don't know the time the appointment
#was booked for
def calculateHour(timestamp):
    timestamp = str(timestamp)
    hour = int(timestamp[11:13])
    minute = int(timestamp[14:16])
    second = int(timestamp[17:])
    return round(hour + minute/60 + second/3600)

df['HourOfTheDay'] = df.ScheduledDay.apply(calculateHour)

#add in month of appointment (possible weather affects)
def calculateMonth(timestamp):
    timestamp = str(timestamp)
    month = int(timestamp[5:7])
    return round(month)

df['MonthofAppointment'] = df.AppointmentDay.apply(calculateMonth)
print(df.head())

#check for missing values, and check unique values for each feature in dataset
print('Age:',sorted(df.Age.unique()))
print('Gender:',df.Gender.unique())
print('Diabetes:',df.Diabetes.unique())
print('Alcoholism:',df.Alcoholism.unique())
print('Hypertension:',df.Hypertension.unique())
print('Handicap:',df.Handicap.unique())
print('Scholarship:',df.Scholarship.unique())
print('SMS_received:',df.SMS_received.unique())
print('HourOfTheDay:', sorted(df.HourOfTheDay.unique()))
print('DaysToAppointment:', sorted(df.DaysToAppointment.unique()))
print('MonthofAppointment:', sorted(df.MonthofAppointment.unique()))

#Ages -1 and above 100 are going to be considered outliers
#DaystoAppointment -1 and -6 are also outliers
df = df[(df.Age >= 0) & (df.Age <= 100) & (df.DaysToAppointment >= 0)]
df_out = df['No-show']
df_in = df.drop(['No-show'], axis=1)

print('there are ',df_in.shape[0],' people in the data set')

#The appointments are only for the months of April, May and June, so we are going to remove that feature
#We will remove the features with dates and times
df_in = df_in.drop(['ScheduledDay','AppointmentDay','MonthofAppointment'], axis=1)
df_in2 = df_in.drop(['Neighbourhood'], axis=1)#we may need to drop the Neighbourhood feature too

#Now we need to encode the following features: Gender, and Neighbourhood (the latter will need hot encoding)

from sklearn.preprocessing import LabelEncoder#this is used for encoding categorical values to 0,1,2... (in order)
gender_le = LabelEncoder()#encoding M and F into numbers in Gender
neigh_le = LabelEncoder()#encoding hospital into numbers
noshow_le = LabelEncoder()#encoding Yes or No: noshow values into numbers

X = df_in.values
X2 = df_in2.values#in case dropping the Neighbourhood feature helps

Y_out = df_out.values

X[:, 0] = gender_le.fit_transform(X[:, 0])#encoding gender feature so male->1, female->0
X2[:, 0] = X[:, 0]#encoding gender feature so male->1, female->0
print(X[0:5, :])

X[:, 2] = neigh_le.fit_transform(X[:, 2])#
#data_label_encoded2 = embarked_le.fit_transform(df['Embarked'])
#df['Embarked'] = data_label_encoded2
#print(df.head())
print('The first 5 rows of our dataset without one hot encoding are:')
print(X[0:5, :])
print('Neighbourhood:', sorted(df_in.Neighbourhood.unique()))
print(len(sorted(df_in.Neighbourhood.unique())))

from sklearn.preprocessing import OneHotEncoder#this is used for unordered categorical values
ohe = OneHotEncoder(categorical_features=[2])#we need to encode Neighbourhood because there it has no inherent order
X = ohe.fit_transform(X).toarray()#note, when you do this the one hot encoding goes to the first columns of the array
X = np.delete(X, 0, axis=1)#we delete the first column of X because the information can be inferred by the second
    # two columns

Y_out = noshow_le.fit_transform(Y_out)#encoding gender feature so Malignant (Yes)->1, Benign (No)->0
print(Y_out[0:10])


#Scaling
stdsc = StandardScaler()
X_std = stdsc.fit_transform(X)#standardize the values of our input set
X2_std = stdsc.fit_transform(X2)#standardize the values of our input set

print(X_std.shape,Y_out.shape)
print(X.shape)

#now we are going to split our training data into training and testing data, the reason for this is so that we can
#test for overfitting and such
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test =\
    train_test_split(X_std, Y_out,\
    test_size = 0.2,\
    random_state = 1,\
    stratify = Y_out)#this makes sure the training and test datasets have same class proportions
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2_std, Y_out,\
        test_size = 0.2,\
        random_state = 1,\
        stratify = Y_out)#this makes sure the training and test datasets have same class proportions

#######################################################################################################
#######################################################################################################
#######################################################################################################

#Logistic Regression Estimator

pipe_lr = make_pipeline(PCA(), LogisticRegression(random_state=0))
print(pipe_lr.get_params().keys())
param_range = [.1, .5, 1, 10, 50]#regularization parameter for logistic regression

def lrpca(X_train, y_train):
    n_components=np.arange(3,X_train.shape[1]+1,1)#the number of components PCA reduces the data to
    param_grid = [{'pca__n_components': n_components, 'logisticregression__C': param_range}]
    gslr = GridSearchCV(estimator= pipe_lr,\
        param_grid=param_grid,\
        scoring='accuracy',\
        cv=10)

    gslr = gslr.fit(X_train, y_train)

    print('For Logistic Regression')
    print('The best regulation parameter choice is: ',gslr.best_params_)
    print('The best score is: %.3f' % (gslr.best_score_))
    lreg_best = gslr.best_estimator_ #uses the best parameter values found via grid search
    return lreg_best

print('When not using one hot encoding')
lrbest = lrpca(X_train2, y_train2)

scores_lreg = cross_val_score(estimator=lrbest, X=X2_std, y=Y_out, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_lreg), np.std(scores_lreg)))
lrbest.fit(X_train2, y_train2)
print('Test accuracy: %.3f' % lrbest.score(X_test2, y_test2))




